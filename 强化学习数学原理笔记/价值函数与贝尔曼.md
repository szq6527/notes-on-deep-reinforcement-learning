# 强化学习笔记：价值函数与贝尔曼方程

## State Value（状态价值函数）

- **状态价值（State Value）**：在遵循策略 $\pi$ 的前提下，从状态 $s$ 出发所能获得的期望回报。
- **回报（Return）**：从某时刻开始，后续所有奖励的折扣总和，是一条具体轨迹的产出。
- **价值（Value）**：对所有可能轨迹的回报求期望，是一个统计量。

> **状态价值函数的定义**：
> 
> $$v_\pi(s) = \mathbb{E}_\pi \left[ G_t \mid S_t = s \right]$$
> 
> 其中：
> - $G_t = \sum_{k=0}^\infty \gamma^k R_{t+k+1}$ 表示从时刻 $t$ 开始的总折扣回报；
> - $\mathbb{E}_\pi[\cdot]$ 表示在策略 $\pi$ 下的期望。

---

## Bellman Equation（贝尔曼方程）

贝尔曼方程建立了当前状态价值与后继状态价值之间的递归关系。

### 状态价值函数的贝尔曼展开

$$
v_\pi(s) = \mathbb{E}_\pi \left[ R_{t+1} + \gamma G_{t+1} \mid S_t = s \right]
$$

进一步展开为：

$$
v_\pi(s) = \sum_a \pi(a \mid s) \left( \mathcal{R}_s^a + \gamma \sum_{s'} \mathcal{P}_{ss'}^a v_\pi(s') \right)
$$

也可以写成期望形式：

$$
v_\pi(s) = \sum_a \pi(a \mid s) \mathbb{E} \left[ R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t = s, A_t = a \right]
$$

其中：
- $\mathcal{R}_s^a = \mathbb{E}[R_{t+1} \mid S_t = s, A_t = a]$
- $\mathcal{P}_{ss'}^a = p(s' \mid s, a)$

---

## 矩阵形式的贝尔曼方程

定义：
- 价值向量 $\mathbf{v}_\pi$，其中第 $s$ 项为 $v_\pi(s)$；
- 奖励向量 $\mathbf{r}_\pi$，其中第 $s$ 项为 $r_\pi(s) = \sum_a \pi(a \mid s) \mathcal{R}_s^a$；
- 状态转移矩阵 $\mathbf{P}_\pi$，其中第 $(s, s')$ 项为 $P_\pi(s, s') = \sum_a \pi(a \mid s) \mathcal{P}_{ss'}^a$。

贝尔曼方程变为：

$$
\mathbf{v}_\pi = \mathbf{r}_\pi + \gamma \mathbf{P}_\pi \mathbf{v}_\pi
$$

求解形式：

$$
\mathbf{v}_\pi = (I - \gamma \mathbf{P}_\pi)^{-1} \mathbf{r}_\pi
$$

---

## 价值函数的迭代求解方法

### 策略评估（Policy Evaluation）

从初始值 $\mathbf{v}^{(0)}$ 开始迭代：

$$
\mathbf{v}^{(k+1)} = \mathbf{r}_\pi + \gamma \mathbf{P}_\pi \mathbf{v}^{(k)}
$$

直到 $\| \mathbf{v}^{(k+1)} - \mathbf{v}^{(k)} \|_\infty < \epsilon$。

### 策略迭代（Policy Iteration）

1. 初始化策略 $\pi_0$；
2. 评估当前策略 $v_{\pi_k}$；
3. 改进策略：

$$
\pi_{k+1}(s) = \arg\max_a \left( \mathcal{R}_s^a + \gamma \sum_{s'} \mathcal{P}_{ss'}^a v_{\pi_k}(s') \right)
$$

4. 如果 $\pi_{k+1} = \pi_k$，则停止，否则返回第 2 步。

### 值迭代（Value Iteration）

$$
v^{(k+1)}(s) = \max_a \left( \mathcal{R}_s^a + \gamma \sum_{s'} \mathcal{P}_{ss'}^a v^{(k)}(s') \right)
$$

最终得到：

$$
\pi^*(s) = \arg\max_a \left( \mathcal{R}_s^a + \gamma \sum_{s'} \mathcal{P}_{ss'}^a v^*(s') \right)
$$

---

## Action Value（动作价值函数）

定义：

$$
q_\pi(s, a) = \mathbb{E}_\pi \left[ G_t \mid S_t = s, A_t = a \right] = \mathcal{R}_s^a + \gamma \sum_{s'} \mathcal{P}_{ss'}^a v_\pi(s')
$$

状态价值与动作价值之间关系：

$$
v_\pi(s) = \sum_a \pi(a \mid s) q_\pi(s, a)
$$

$$
q_\pi(s, a) = \mathcal{R}_s^a + \gamma \sum_{s'} \mathcal{P}_{ss'}^a \sum_{a'} \pi(a' \mid s') q_\pi(s', a')
$$

---

## 贝尔曼最优方程（Bellman Optimality Equation）

最优状态价值函数：

$$
v^*(s) = \max_a q^*(s, a)
$$

最优动作价值函数：

$$
q^*(s, a) = \mathcal{R}_s^a + \gamma \sum_{s'} \mathcal{P}_{ss'}^a \max_{a'} q^*(s', a')
$$

---

## 不动点定理与收缩映射

### 收缩映射定义

若映射 $f$ 满足：

$$
d(f(x), f(y)) \leq k \cdot d(x, y),\quad 0 \leq k < 1
$$

则称其为收缩映射。

### 巴拿赫不动点定理

在完备度量空间中，收缩映射 $f$ 有唯一不动点 $x^*$，迭代必收敛于该点：

$$
f(x^*) = x^*
$$

### 贝尔曼最优算子

定义：

$$
(\mathcal{T}v)(s) = \max_a \left( \mathcal{R}_s^a + \gamma \sum_{s'} \mathcal{P}_{ss'}^a v(s') \right)
$$

$\mathcal{T}$ 是 $\gamma$-收缩映射，满足：

- 存在唯一不动点 $v^*$；
- 迭代 $v^{(k+1)} = \mathcal{T}v^{(k)}$ 收敛于 $v^*$。

### 最优策略

$$
\pi^*(s) = \arg\max_a q^*(s, a) = \arg\max_a \left( \mathcal{R}_s^a + \gamma \sum_{s'} \mathcal{P}_{ss'}^a v^*(s') \right)
$$

---

