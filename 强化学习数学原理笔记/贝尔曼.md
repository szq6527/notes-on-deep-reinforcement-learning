## state value 
- state value 是对多个可能的return求期望
- 状态价值函数定义为：

状态价值函数定义为：

$$
v_\pi(s) = \mathbb{E} \left[ G_t \mid S_t = s \right]
$$

其中：
- \( v_\pi(s) \)：表示在策略 \( \pi \) 下，从状态 \( s \) 出发的期望回报
- \( G_t \)：从时刻 \( t \) 开始的累计折扣奖励，即 \( G_t = \sum_{k=0}^\infty \gamma^k R_{t+k+1} \)
- \( \mathbb{E}[\cdot] \)：期望符号，表示对所有可能路径的加权平均
- 代表状态的价值
- return 是单个 state value是一个期望

## 贝尔曼公式的推导——不同状态下的state value之间的关系
- 注意轨迹的表示方法 
- 递归形式 
- $$
\mathbb{E}[G_{t+1} \mid S_t = s]
= \sum_{s'} \mathbb{E}[G_{t+1} \mid S_t = s, S_{t+1} = s'] \cdot p(s' \mid s)
$$

$$
= \sum_{s'} \mathbb{E}[G_{t+1} \mid S_{t+1} = s'] \cdot p(s' \mid s)
$$

$$
= \sum_{s'} v_\pi(s') \cdot p(s' \mid s)
$$

$$
= \sum_{s'} v_\pi(s') \cdot \sum_a p(s' \mid s, a) \cdot \pi(a \mid s)
$$

###  状态价值函数的贝尔曼展开

状态价值函数定义为：

$$
v_\pi(s) = \mathbb{E}[R_{t+1} \mid S_t = s] + \gamma \mathbb{E}[G_{t+1} \mid S_t = s]
$$

继续展开，得：

$$
v_\pi(s) = \sum_a \pi(a \mid s) \sum_r p(r \mid s, a) r + \gamma \sum_a \pi(a \mid s) \sum_{s'} p(s' \mid s, a) v_\pi(s')
$$

这是 immediate reward 与 future reward 的加权平均形式。

进一步合并为：

$$
v_\pi(s) = \sum_a \pi(a \mid s) \left[
\sum_r p(r \mid s, a) r + \gamma \sum_{s'} p(s' \mid s, a) v_\pi(s')
\right], \quad \forall s \in \mathcal{S}
$$

## 矩阵形式的贝尔曼公式
从贝尔曼方程出发：
\[
v_\pi(s) = \sum_a \pi(a \mid s) \left[
\sum_r p(r \mid s, a) r + \gamma \sum_{s'} p(s' \mid s, a) v_\pi(s')
\right]
\]

将内层拆分并换序：

\[
v_\pi(s) = \sum_a \pi(a \mid s) \sum_r p(r \mid s, a) r + \gamma \sum_a \pi(a \mid s) \sum_{s'} p(s' \mid s, a) v_\pi(s')
\]

定义期望奖励：
\[
r_\pi(s) = \sum_a \pi(a \mid s) \sum_r p(r \mid s, a) r
\]

定义策略下转移概率：
\[
P_\pi(s, s') = \sum_a \pi(a \mid s) p(s' \mid s, a)
\]

代入得：
\[
v_\pi(s) = r_\pi(s) + \gamma \sum_{s'} P_\pi(s, s') v_\pi(s')
\]

写成向量形式：
\[
\mathbf{v}_\pi = \mathbf{r}_\pi + \gamma \mathbf{P}_\pi \mathbf{v}_\pi
\]

移项得线性方程组：
\[
(I - \gamma \mathbf{P}_\pi) \mathbf{v}_\pi = \mathbf{r}_\pi
\]

## 值函数的迭代求解方法

我们要解的是贝尔曼方程的矩阵形式：
\[
\mathbf{v}_\pi = \mathbf{r}_\pi + \gamma \mathbf{P}_\pi \mathbf{v}_\pi
\]
或者：
\[
(I - \gamma \mathbf{P}_\pi) \mathbf{v}_\pi = \mathbf{r}_\pi
\]

直接求逆代价高，所以采用**迭代法**逼近 \(\mathbf{v}_\pi\)。

---

### 策略评估的迭代算法（Policy Evaluation）

令初始值 \(\mathbf{v}^{(0)} = \mathbf{0}\)，迭代更新公式为：

\[
\mathbf{v}^{(k+1)} = \mathbf{r}_\pi + \gamma \mathbf{P}_\pi \mathbf{v}^{(k)}
\]

反复迭代，直到收敛：

\[
\|\mathbf{v}^{(k+1)} - \mathbf{v}^{(k)}\| < \epsilon
\]

这是对：
\[
\mathbf{v}_\pi = \sum_{t=0}^\infty \gamma^t \mathbf{P}_\pi^t \mathbf{r}_\pi
\]
的数值逼近。

---

### 策略迭代（Policy Iteration）

1. **初始化策略** \(\pi_0\)
2. **策略评估**：使用上面的迭代公式求 \(v_{\pi_k}\)
3. **策略改进**：对每个状态 \(s\)，更新策略：
   \[
   \pi_{k+1}(s) = \arg\max_a \left[ \sum_r p(r \mid s, a) r + \gamma \sum_{s'} p(s' \mid s, a) v_{\pi_k}(s') \right]
   \]
4. 若策略不变则停止，否则回到第 2 步

---

### 值迭代（Value Iteration）

值迭代直接融合策略评估和改进：

\[
v^{(k+1)}(s) = \max_a \left[ \sum_r p(r \mid s, a) r + \gamma \sum_{s'} p(s' \mid s, a) v^{(k)}(s') \right]
\]

反复执行直到收敛，再从最终值函数导出最优策略：

\[
\pi^*(s) = \arg\max_a \left[ \sum_r p(r \mid s, a) r + \gamma \sum_{s'} p(s' \mid s, a) v^*(s') \right]
\]

## action value
- 就是state value中括号后面的东西
- action value与state value是可以互相推导的

# 贝尔曼最优公式
## 最优策略的定义 