\documentclass[12pt, a4paper]{article}

% --- 包的设置 (Preamble) ---
\usepackage[UTF8]{ctex} % 中文支持
\usepackage{amsmath, amssymb, amsfonts} % AMS数学公式包
\usepackage{geometry} % 页面布局
\usepackage{graphicx} % 插入图片
\usepackage{booktabs} % 专业表格
\usepackage{enumitem} % 自定义列表
\usepackage{hyperref} % 超链接
\usepackage{xcolor} % 颜色

% --- 页面布局设置 ---
\geometry{a4paper, left=2.5cm, right=2.5cm, top=2.5cm, bottom=2.5cm}

% --- 超链接设置 ---
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={TD学习笔记},
    pdfpagemode=FullScreen,
}

% --- 文档标题 ---
\title{\textbf{强化学习：时序差分(TD)学习的状态价值估计}}
\author{szq (根据PPT内容整理)}
\date{\today}

% --- 文档开始 ---
\begin{document}

\maketitle
\tableofcontents
\newpage

\section{TD学习简介}
时序差分（Temporal Difference, TD）学习是强化学习中的一个核心概念，它结合了蒙特卡洛（Monte Carlo, MC）方法和动态规划（Dynamic Programming, DP）的思想。与MC方法类似，TD学习能直接从原始经验中学习，而无需环境的完整模型。与DP类似，TD学习使用其他已学习到的估计值来更新当前的估计值，这个过程被称为\textbf{自举（bootstrapping）}。本文档将重点介绍用于评估给定策略 $\pi$ 的状态价值函数 $v_\pi$ 的基本TD算法。

\section{TD学习算法}

\subsection{算法所需数据}
TD算法通过智能体遵循给定策略 $\pi$ 所产生的经验进行学习。这些经验是状态、奖励和动作的序列，我们从中提取转移样本。在每个时间步 $t$，算法需要一个样本元组 $(\vect{s_t, r_{t+1}, s_{t+1}})$ 来执行一次更新。
算法的最终目标是使估计的价值函数 $v_t(s)$ 收敛到所有状态 $s$ 的真实价值函数 $v_\pi(s)$。

\subsection{算法描述}
TD算法迭代地更新所访问过的状态 $s_t$ 的价值估计。在任意时间步 $t$，更新规则由以下两个公式定义：

\begin{enumerate}
    \item 对于在时间 $t$ 访问到的状态 $s_t$：
    \[
    v_{t+1}(s_t) = v_t(s_t) - \alpha_t(s_t) \Big[ v_t(s_t) - \big(r_{t+1} + \gamma v_t(s_{t+1})\big) \Big] \label{eq:update}
    \]
    \item 对于所有其他未被访问的状态 $s \neq s_t$：
    \[
    v_{t+1}(s) = v_t(s), \quad \forall s \neq s_t
    \]
\end{enumerate}

其中：
\begin{itemize}[leftmargin=*]
    \item $v_t(s)$ 是状态 $s$ 在时间步 $t$ 的估计价值。
    \item $\alpha_t(s_t)$ 是在时间 $t$ 进行更新的学习率（或步长）。它可以是一个小的正常数，也可以是一个随时间递减的函数。
    \item $\gamma$ 是折扣因子。
\end{itemize}

\textbf{核心要点：}
\begin{itemize}
    \item 在每一步，只有最近访问过的状态 $s_t$ 的价值被更新，所有其他状态的价值保持不变。
    \item 在实际表述中，第二个公式通常被省略，因为其含义是默认的。
\end{itemize}

\section{TD算法的属性与解读}
为了更好地理解TD更新的机制，我们可以对核心更新规则进行标注，并将其分解为几个关键部分。

\subsection{TD目标与TD误差}
我们可以重写更新规则，以突出两个关键术语：\textbf{TD目标（TD Target）}和\textbf{TD误差（TD Error）}。
\[
\underbrace{v_{t+1}(s_t)}_{\text{新估计}} = \underbrace{v_t(s_t)}_{\text{当前估计}} - \alpha_t(s_t) \underbrace{\Big[ v_t(s_t) - \overbrace{\big(r_{t+1} + \gamma v_t(s_{t+1})\big)}^{\text{TD目标 } \bar{v}_t} \Big]}_{\text{TD误差 } \delta_t}
\]
这两个组成部分被正式定义为：
\begin{itemize}
    \item \textbf{TD目标 ($\bar{v}_t$):} $\bar{v}_t \doteq r_{t+1} + \gamma v_t(s_{t+1})$ 被称为TD目标。它是对 $s_t$ 价值的一个自举估计，由即时奖励 $r_{t+1}$ 和下一状态价值的折扣估计 $v_t(s_{t+1})$ 构成。

    \item \textbf{TD误差 ($\delta_t$):} $\delta_t \doteq v_t(s_t) - \bar{v}_t = v_t(s_t) - [r_{t+1} + \gamma v_t(s_{t+1})]$ 被称为TD误差。它衡量了当前估计 $v_t(s_t)$ 与信息更丰富（因为它包含了真实奖励$r_{t+1}$）的TD目标 $\bar{v}_t$ 之间的差异。
\end{itemize}
因此，更新规则可以被看作是朝着TD目标 $\bar{v}_t$ 的方向，对当前估计 $v_t(s_t)$ 进行调整，调整的幅度由学习率 $\alpha_t$ 控制。

\subsection{为什么称 \texorpdfstring{$\bar{v}_t$}{v_bar_t} 为TD目标？}
使用“目标”一词是因为该算法不断地将价值估计 $v(s_t)$ 推向 $\bar{v}_t$。我们可以从数学上证明这一点。
从更新规则出发：
\begin{align*}
v_{t+1}(s_t) &= v_t(s_t) - \alpha_t(s_t) [v_t(s_t) - \bar{v}_t] \\
% 从两边减去目标值
\implies v_{t+1}(s_t) - \bar{v}_t &= v_t(s_t) - \bar{v}_t - \alpha_t(s_t) [v_t(s_t) - \bar{v}_t] \\
% 提取公因式
\implies v_{t+1}(s_t) - \bar{v}_t &= [1 - \alpha_t(s_t)] [v_t(s_t) - \bar{v}_t] \\
% 取绝对值
\implies |v_{t+1}(s_t) - \bar{v}_t| &= |1 - \alpha_t(s_t)| |v_t(s_t) - \bar{v}_t|
\end{align*}
如果我们假设学习率 $\alpha_t(s_t)$ 是一个小的正数，满足 $0 < \alpha_t(s_t) < 1$，那么我们有 $0 < 1 - \alpha_t(s_t) < 1$。
因此，
\[
|v_{t+1}(s_t) - \bar{v}_t| \leq |v_t(s_t) - \bar{v}_t|
\]
这个不等式表明，每一步更新都会减小估计值 $v(s_t)$ 与TD目标 $\bar{v}_t$ 之间的差距，从而有效地将估计值拉向目标。

\subsection{TD误差的解读}
TD误差 $\delta_t$ 是TD学习中的一个核心量，它有几个重要的解释：
\begin{itemize}[leftmargin=*, wide]
    \item \textbf{连续时间步估计之间的差异。} 误差 $\delta_t$ 量化了状态 $s_t$ 的价值估计与后继状态 $s_{t+1}$ 的价值估计之间的不一致性。

    \item \textbf{衡量当前估计 $v_t$ 与真实价值 $v_\pi$ 之间的“缺陷”。} 为了理解这一点，我们可以考虑真实价值函数 $v_\pi$ 的\textit{贝尔曼误差}：
    \[
    \delta_{\pi, t} \doteq v_\pi(s_t) - [r_{t+1} + \gamma v_\pi(s_{t+1})]
    \]
    根据 $v_\pi$ 的贝尔曼期望方程的定义，这个量的期望值为零：
    \[
    \mathbb{E}_\pi[\delta_{\pi, t} | S_t = s_t] = v_\pi(s_t) - \mathbb{E}_\pi[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s_t] = 0
    \]
    这说明，如果我们的估计 $v_t$ 是完美的（即 $v_t = v_\pi$），那么期望的TD误差将为零。反之，一个（在期望意义上）非零的TD误差表明我们当前的价值函数 $v_t$ 是不准确的，需要更新。

    \item \textbf{创新（Innovation）。} TD误差可以被解释为从经验样本 $(s_t, r_{t+1}, s_{t+1})$ 中获得的“新信息”或“意外”。它代表了TD目标中未被当前估计 $v_t(s_t)$ 所预测到的部分。这个新信息随后被用来改进估计。
\end{itemize}

\section{算法的范围与局限性}
理解这里描述的基础TD算法的适用范围至关重要。
\begin{itemize}
    \item 这种形式的TD算法\textbf{仅仅是用来评估一个给定策略 $\pi$ 的状态价值}。它是一种\textit{策略评估}算法。
    \item 它不直接估计动作价值 ($q_\pi(s,a)$)。
    \item 它不寻找最优策略。它仅仅评估用于生成数据的那个策略的好坏。
\end{itemize}
尽管有这些局限性，这个算法是极其基础和重要的。TD误差和自举等核心概念构成了更高级算法（如Q-Learning和SARSA）的基础，而这些算法能够估计动作价值并寻找最优策略。

\newpage
\section{TD算法的数学原理}
从数学角度看，TD算法究竟在做什么？简而言之：\textbf{它在求解给定策略 $\pi$ 的贝尔曼方程}。下面我们将详细阐述这个过程。

\subsection{贝尔曼期望方程}
首先，我们需要理解TD算法的目标方程。对于一个给定的策略 $\pi$，其状态价值函数 $v_\pi(s)$ 定义为从状态 $s$ 出发能获得的期望折扣回报。
这个定义可以递归地展开，得到贝尔曼方程。其中一种重要的形式是\textbf{贝尔曼期望方程 (Bellman expectation equation)}：
\[
v_\pi(s) = \mathbb{E}_\pi [R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s], \quad s \in \mathcal{S}.
\]
这个方程表明，一个状态的真实价值等于在该状态下采取策略 $\pi$ 后，获得的即时奖励与下一状态的折扣价值的期望值。TD算法的核心目标就是找到一个价值函数 $v$，使其满足这个方程。

\subsection{使用Robbins-Monro算法求解}
为了求解贝尔曼期望方程，我们可以将其转化为一个随机近似（Stochastic Approximation）问题。

\subsubsection{问题重构}
我们可以定义一个函数 $g(v(s))$，使得当 $v(s)$ 是真实价值 $v_\pi(s)$ 时，该函数值为零：
\[
g(v(s)) = v(s) - \mathbb{E}_\pi [R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t=s]
\]
因此，求解贝尔曼期望方程就等价于寻找函数 $g(v(s))$ 的根，即求解 $g(v(s))=0$。

在实际应用中，我们无法计算期望 $\mathbb{E}[\cdot]$，因为我们没有环境模型。我们只能通过与环境交互来获得具体的样本，如奖励 $r$ 和下一状态 $s'$。因此，我们只能得到一个带有噪声的观测值 $\tilde{g}(v(s))$:
\[
\tilde{g}(v(s)) = v(s) - [r + \gamma v_\pi(s')]
\]
这个带噪观测可以分解为：
\[
\tilde{g}(v(s)) = \underbrace{\left(v(s) - \mathbb{E}[R + \gamma v_\pi(S')|s]\right)}_{g(v(s))} + \underbrace{\left(\mathbb{E}[R + \gamma v_\pi(S')|s] - [r + \gamma v_\pi(s')]\right)}_{\eta}
\]
其中，$\eta$ 是一个零均值的噪声项，即 $\mathbb{E}[\eta]=0$。

\subsubsection{理想化的RM算法}
有了这个设定，我们就可以使用经典的\textbf{Robbins-Monro (RM) 算法}来寻找 $g(v(s))=0$ 的解。RM算法的迭代格式如下：
\begin{align}
v_{k+1}(s) &= v_k(s) - \alpha_k \tilde{g}(v_k(s)) \nonumber \\
           &= v_k(s) - \alpha_k \left( v_k(s) - [r_k + \gamma v_\pi(s'_k)] \right), \quad k=1,2,3,\dots
\end{align}
其中，$v_k(s)$ 是对 $v_\pi(s)$ 的第 $k$ 次估计，$r_k$ 和 $s'_k$ 是在第 $k$ 步获得的样本。

然而，上述RM算法依赖于两个非常强的假设：
\begin{enumerate}
    \item 我们有一个经验样本集合 $\{(s, r, s')\}$。
    \item 我们\textbf{已经知道了}下一状态的真实价值 $v_\pi(s')$。
\end{enumerate}
第二个假设在现实中是不成立的，因为 $v_\pi$ 正是我们要计算的目标。

\subsubsection{从RM算法到TD算法的修正}
为了让算法变得实用，我们需要移除上述不切实际的假设。这引出了两个关键的修正，从而将理想化的RM算法转变为我们所熟知的TD算法：
\begin{itemize}
    \item \textbf{使用序列样本：} 将样本集从独立的 $\{(s,r,s')\}$ 变为在同一个episode中连续产生的序列样本 $\{(s_t, r_{t+1}, s_{t+1})\}$，这使得算法可以在线学习。
    \item \textbf{使用当前估计值替代真实值 (自举)：} 这是最重要的一步。因为我们不知道真实的 $v_\pi(s')$，我们用当前对它的\textbf{估计值} $v_k(s')$ 来代替它。
\end{itemize}

通过第二个修正，我们将RM更新公式中的未知项 $v_\pi(s'_k)$ 替换为已知的当前估计值 $v_k(s'_k)$（在TD的语境下，即 $v_t(s_{t+1})$）。这样，更新公式就变成了：
\[
v_{t+1}(s_t) = v_t(s_t) - \alpha_t \left( v_t(s_t) - [r_{t+1} + \gamma \boldsymbol{v_t(s_{t+1})}] \right)
\]
这正是我们在前面章节中介绍的TD学习算法的更新规则。通过这种方式，TD算法可以被严谨地看作是求解贝尔曼期望方程的一种随机近似方法。


\end{document}